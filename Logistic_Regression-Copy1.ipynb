# this is ipynb file.
### Load the Data and Libraries

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
plt.style.use("ggplot")
%matplotlib inline

from pylab import rcParams
rcParams['figure.figsize'] = 12, 8

data = pd.read_csv('DMV_Written_Tests.csv')
data.head()

data.info()

scores =data[['DMV_Test_1','DMV_Test_2']].values
results = data['Results'].values

#### T Visualize the Data

passed = (results ==1).reshape(100,1)
failed = (results ==0). reshape(100,1)
ax = sns.scatterplot(x = scores[passed[: , 0] , 0],
                     y = scores[passed[: , 0] , 1],
                     marker ='^',
                     color ='green',
                     s=60)
sns.scatterplot(x=scores[failed[: , 0] , 0],
                y=scores[failed[: , 0], 1],
                marker ='X',
                color ='red',
                s=60)
ax.set(xlabel='DMV WRITTEN TEST 1' , ylabel ='DMV WRITTEN TEST 2')
ax.legend(['passed', 'failed'])
plt.show();

###Define the Logistic Sigmoid Function  ðœŽ(ð‘§) 
##ðœŽ(ð‘§)=11+ð‘’âˆ’ð‘§


def logistic_function(x):
    return 1/(1 + np.exp(-x))
logistic_function(0)    


####Compute the Cost Function ð½(ðœƒ) and GradientÂ¶
#The objective of logistic regression is to minimize the cost function

#ð½(ðœƒ)=âˆ’1ð‘šâˆ‘ð‘–=1ð‘š[ð‘¦(ð‘–)ð‘™ð‘œð‘”(â„Žðœƒ(ð‘¥(ð‘–)))+(1âˆ’ð‘¦(ð‘–))ð‘™ð‘œð‘”(1âˆ’(â„Žðœƒ(ð‘¥(ð‘–)))]
 
#where the gradient of the cost function is given by

#âˆ‚ð½(ðœƒ)âˆ‚ðœƒð‘—=1ð‘šâˆ‘ð‘–=1ð‘š(â„Žðœƒ(ð‘¥(ð‘–))âˆ’ð‘¦(ð‘–))ð‘¥(ð‘–)ð‘—
    
def compute_cost(theta , x  ,y):
    m=len(y)
    y_pred=logistic_function(np.dot(x,theta))
    error=(y*np.log(y_pred))+ (1-y)*np.log(1-y_pred)
    cost= -1/m *sum(error)
    gradient =1/m *(np.dot(x.transpose(),(y_pred -y)))
    return cost[0],gradient
    
    
###Cost and Gradient at Initialization

mean_scores=np.mean(scores , axis =0)
std_scores=np.std(scores, axis=0)
scores=(scores- mean_scores) /std_scores
rows=scores.shape[0]
cols=scores.shape[1]
X=np.append(np.ones((rows,1)),scores , axis=1)
y=results.reshape(rows,1)
theta_init=np.zeros((cols+1 ,1 ))
cost,gradient =compute_cost(theta_init ,X,y)
print("COST AT INITIALIZATION" ,cost)
print("GRADIENT AT INITIALIZATION" ,gradient)

###Gradient Descent

def gradient_descent (X,y,theta,alpha, iterations):
    costs=[]
    for i in range(iterations):
        cost , gradient =compute_cost(theta , X ,y)
        theta -=(alpha * gradient)
        costs.append(cost)
    return theta, costs  
    
theta ,costs=gradient_descent(X,y,theta_init,1, 200)

print('Theta after running gradient descent', theta)
print('Resulting cost:', costs[-1])


### Plotting the Convergence of  ð½(ðœƒ)

plt.plot(costs)
plt.xlabel("Iteraions")
plt.ylabel("$J(\Theta)$")
plt.title("Values of Cost Function over Iterations  over Gradient Descent");

### Plotting the decision boundary
#â„Žðœƒ(ð‘¥)=ðœŽ(ð‘§), where ðœŽ is the logistic sigmoid function and ð‘§=ðœƒð‘‡ð‘¥
#When â„Žðœƒ(ð‘¥)â‰¥0.5 the model predicts class "1":

#âŸ¹ðœŽ(ðœƒð‘‡ð‘¥)â‰¥0.5
#âŸ¹ðœƒð‘‡ð‘¥â‰¥0 predict class "1"

#Hence, ðœƒ1+ðœƒ2ð‘¥2+ðœƒ3ð‘¥3=0 is the equation for the decision boundary, giving us

#ð‘¥3=âˆ’(ðœƒ1+ðœƒ2ð‘¥2)ðœƒ3

ax = sns.scatterplot(x = X[passed[: , 0] , 1],
                     y = X[passed[: , 0] , 2],
                     marker ='^',
                     color ='green',
                     s=60)
sns.scatterplot(x = X[failed[: , 0] , 1],
                y = X[failed[: , 0], 2],
                marker ='X',
                color ='red',
                s=60)
ax.set(xlabel='DMV WRITTEN TEST 1 SCORES' , ylabel ='DMV WRITTEN TEST 2 SCORES')
ax.legend(['passed', 'failed'])
x_boundary=np.array([np.min(X[: ,1]) , np.max(X[:,1])])
y_boundary=-(theta[0]+ theta[1]*x_boundary)/theta[2]
sns.lineplot(x=x_boundary , y=y_boundary , color='blue')
plt.show();


### Predictions using the optimized ðœƒ valuesÂ¶
#â„Žðœƒ(ð‘¥)=ð‘¥ðœƒ

def predict(theta ,X):
    results=X.dot(theta)
    return results>0
p=predict(theta ,X)
print("Training Accuracy :", sum(p==y)[0] ,"%")


test=np.array([50,79])
test=(test - mean_scores) / std_scores
test= np.append(np.ones(1) , test)
probability = logistic_function(test.dot(theta))
print("A person who scores 50 & 79 on their DMV written test has a"  ,np.round(probability[0] ,2) , "probability of passing")
